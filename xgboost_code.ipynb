{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [0, 1, 5, 7, 11, 12, 13]+[i for i in range(16, 33)]\n",
    "train_data = pd.read_csv('train.csv', header=0, usecols=cols)\n",
    "test_data = pd.read_csv('test.csv', header=0, usecols=cols[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicate data because label 1 only has 1 entry\n",
    "train_data = train_data.append([train_data[train_data.label == 1]]*9, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[['num-comments', 'feedback-karma', 'ratings-given', 'ratings-received',\n",
    "                'num-authors', 'prev-games', 'fun-average', 'innovation-average', 'theme-average',\n",
    "                'graphics-average', 'audio-average', 'humor-average', 'mood-average', 'fun-rank',\n",
    "                'innovation-rank', 'theme-rank', 'graphics-rank', 'audio-rank', 'humor-rank', 'mood-rank']]\n",
    "y = train_data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = test_data[['num-comments', 'feedback-karma', 'ratings-given', 'ratings-received', \n",
    "                'num-authors', 'prev-games', 'fun-average', 'innovation-average', 'theme-average', \n",
    "                'graphics-average', 'audio-average', 'humor-average', 'mood-average', 'fun-rank', \n",
    "                'innovation-rank', 'theme-rank', 'graphics-rank', 'audio-rank', 'humor-rank', 'mood-rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dtest = xgb.DMatrix(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:0.43367\ttrain-mlogloss:0.43011\n",
      "Multiple eval metrics have been passed: 'train-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until train-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\teval-mlogloss:0.29174\ttrain-mlogloss:0.28105\n",
      "[2]\teval-mlogloss:0.22180\ttrain-mlogloss:0.21357\n",
      "[3]\teval-mlogloss:0.19217\ttrain-mlogloss:0.18337\n",
      "[4]\teval-mlogloss:0.17266\ttrain-mlogloss:0.16392\n",
      "[5]\teval-mlogloss:0.16230\ttrain-mlogloss:0.15243\n",
      "[6]\teval-mlogloss:0.15684\ttrain-mlogloss:0.14410\n",
      "[7]\teval-mlogloss:0.15185\ttrain-mlogloss:0.13872\n",
      "[8]\teval-mlogloss:0.14856\ttrain-mlogloss:0.13469\n",
      "[9]\teval-mlogloss:0.14682\ttrain-mlogloss:0.13162\n",
      "[10]\teval-mlogloss:0.14483\ttrain-mlogloss:0.12896\n",
      "[11]\teval-mlogloss:0.14402\ttrain-mlogloss:0.12644\n",
      "[12]\teval-mlogloss:0.14293\ttrain-mlogloss:0.12456\n",
      "[13]\teval-mlogloss:0.14210\ttrain-mlogloss:0.12295\n",
      "[14]\teval-mlogloss:0.14105\ttrain-mlogloss:0.12089\n",
      "[15]\teval-mlogloss:0.14116\ttrain-mlogloss:0.11944\n",
      "[16]\teval-mlogloss:0.13967\ttrain-mlogloss:0.11823\n",
      "[17]\teval-mlogloss:0.13970\ttrain-mlogloss:0.11733\n",
      "[18]\teval-mlogloss:0.13916\ttrain-mlogloss:0.11635\n",
      "[19]\teval-mlogloss:0.13890\ttrain-mlogloss:0.11533\n",
      "[20]\teval-mlogloss:0.13948\ttrain-mlogloss:0.11432\n",
      "[21]\teval-mlogloss:0.13926\ttrain-mlogloss:0.11348\n",
      "[22]\teval-mlogloss:0.13917\ttrain-mlogloss:0.11281\n",
      "[23]\teval-mlogloss:0.13934\ttrain-mlogloss:0.11185\n",
      "[24]\teval-mlogloss:0.13874\ttrain-mlogloss:0.11099\n",
      "[25]\teval-mlogloss:0.13845\ttrain-mlogloss:0.10978\n",
      "[26]\teval-mlogloss:0.13844\ttrain-mlogloss:0.10876\n",
      "[27]\teval-mlogloss:0.13787\ttrain-mlogloss:0.10775\n",
      "[28]\teval-mlogloss:0.13770\ttrain-mlogloss:0.10682\n",
      "[29]\teval-mlogloss:0.13704\ttrain-mlogloss:0.10592\n",
      "[30]\teval-mlogloss:0.13747\ttrain-mlogloss:0.10516\n",
      "[31]\teval-mlogloss:0.13776\ttrain-mlogloss:0.10430\n",
      "[32]\teval-mlogloss:0.13802\ttrain-mlogloss:0.10365\n",
      "[33]\teval-mlogloss:0.13778\ttrain-mlogloss:0.10296\n",
      "[34]\teval-mlogloss:0.13789\ttrain-mlogloss:0.10239\n",
      "[35]\teval-mlogloss:0.13831\ttrain-mlogloss:0.10175\n",
      "[36]\teval-mlogloss:0.13834\ttrain-mlogloss:0.10085\n",
      "[37]\teval-mlogloss:0.13851\ttrain-mlogloss:0.10007\n",
      "[38]\teval-mlogloss:0.13836\ttrain-mlogloss:0.09957\n",
      "[39]\teval-mlogloss:0.13784\ttrain-mlogloss:0.09884\n",
      "[40]\teval-mlogloss:0.13804\ttrain-mlogloss:0.09810\n",
      "[41]\teval-mlogloss:0.13852\ttrain-mlogloss:0.09749\n",
      "[42]\teval-mlogloss:0.13878\ttrain-mlogloss:0.09688\n",
      "[43]\teval-mlogloss:0.13861\ttrain-mlogloss:0.09631\n",
      "[44]\teval-mlogloss:0.13862\ttrain-mlogloss:0.09569\n",
      "[45]\teval-mlogloss:0.13817\ttrain-mlogloss:0.09526\n",
      "[46]\teval-mlogloss:0.13810\ttrain-mlogloss:0.09452\n",
      "[47]\teval-mlogloss:0.13810\ttrain-mlogloss:0.09399\n",
      "[48]\teval-mlogloss:0.13816\ttrain-mlogloss:0.09336\n",
      "[49]\teval-mlogloss:0.13846\ttrain-mlogloss:0.09286\n",
      "[50]\teval-mlogloss:0.13906\ttrain-mlogloss:0.09213\n",
      "[51]\teval-mlogloss:0.13976\ttrain-mlogloss:0.09154\n",
      "[52]\teval-mlogloss:0.13967\ttrain-mlogloss:0.09103\n",
      "[53]\teval-mlogloss:0.13990\ttrain-mlogloss:0.09051\n",
      "[54]\teval-mlogloss:0.13970\ttrain-mlogloss:0.09005\n",
      "[55]\teval-mlogloss:0.13965\ttrain-mlogloss:0.08950\n",
      "[56]\teval-mlogloss:0.13983\ttrain-mlogloss:0.08913\n",
      "[57]\teval-mlogloss:0.13994\ttrain-mlogloss:0.08869\n",
      "[58]\teval-mlogloss:0.13975\ttrain-mlogloss:0.08822\n",
      "[59]\teval-mlogloss:0.14049\ttrain-mlogloss:0.08775\n",
      "[60]\teval-mlogloss:0.14176\ttrain-mlogloss:0.08724\n",
      "[61]\teval-mlogloss:0.14225\ttrain-mlogloss:0.08685\n",
      "[62]\teval-mlogloss:0.14240\ttrain-mlogloss:0.08633\n",
      "[63]\teval-mlogloss:0.14280\ttrain-mlogloss:0.08571\n",
      "[64]\teval-mlogloss:0.14266\ttrain-mlogloss:0.08539\n",
      "[65]\teval-mlogloss:0.14260\ttrain-mlogloss:0.08496\n",
      "[66]\teval-mlogloss:0.14236\ttrain-mlogloss:0.08461\n",
      "[67]\teval-mlogloss:0.14208\ttrain-mlogloss:0.08395\n",
      "[68]\teval-mlogloss:0.14267\ttrain-mlogloss:0.08368\n",
      "[69]\teval-mlogloss:0.14290\ttrain-mlogloss:0.08318\n",
      "[70]\teval-mlogloss:0.14347\ttrain-mlogloss:0.08278\n",
      "[71]\teval-mlogloss:0.14393\ttrain-mlogloss:0.08248\n",
      "[72]\teval-mlogloss:0.14397\ttrain-mlogloss:0.08205\n",
      "[73]\teval-mlogloss:0.14418\ttrain-mlogloss:0.08155\n",
      "[74]\teval-mlogloss:0.14472\ttrain-mlogloss:0.08121\n",
      "[75]\teval-mlogloss:0.14447\ttrain-mlogloss:0.08053\n",
      "[76]\teval-mlogloss:0.14478\ttrain-mlogloss:0.08023\n",
      "[77]\teval-mlogloss:0.14485\ttrain-mlogloss:0.08002\n",
      "[78]\teval-mlogloss:0.14511\ttrain-mlogloss:0.07970\n",
      "[79]\teval-mlogloss:0.14515\ttrain-mlogloss:0.07928\n",
      "[80]\teval-mlogloss:0.14525\ttrain-mlogloss:0.07908\n",
      "[81]\teval-mlogloss:0.14602\ttrain-mlogloss:0.07871\n",
      "[82]\teval-mlogloss:0.14676\ttrain-mlogloss:0.07828\n",
      "[83]\teval-mlogloss:0.14690\ttrain-mlogloss:0.07803\n",
      "[84]\teval-mlogloss:0.14643\ttrain-mlogloss:0.07773\n",
      "[85]\teval-mlogloss:0.14662\ttrain-mlogloss:0.07743\n",
      "[86]\teval-mlogloss:0.14616\ttrain-mlogloss:0.07708\n",
      "[87]\teval-mlogloss:0.14685\ttrain-mlogloss:0.07669\n",
      "[88]\teval-mlogloss:0.14662\ttrain-mlogloss:0.07615\n",
      "[89]\teval-mlogloss:0.14612\ttrain-mlogloss:0.07590\n",
      "[90]\teval-mlogloss:0.14645\ttrain-mlogloss:0.07556\n",
      "[91]\teval-mlogloss:0.14642\ttrain-mlogloss:0.07524\n",
      "[92]\teval-mlogloss:0.14637\ttrain-mlogloss:0.07501\n",
      "[93]\teval-mlogloss:0.14641\ttrain-mlogloss:0.07464\n",
      "[94]\teval-mlogloss:0.14666\ttrain-mlogloss:0.07434\n",
      "[95]\teval-mlogloss:0.14663\ttrain-mlogloss:0.07412\n",
      "[96]\teval-mlogloss:0.14686\ttrain-mlogloss:0.07369\n",
      "[97]\teval-mlogloss:0.14728\ttrain-mlogloss:0.07331\n",
      "[98]\teval-mlogloss:0.14745\ttrain-mlogloss:0.07291\n",
      "[99]\teval-mlogloss:0.14736\ttrain-mlogloss:0.07249\n"
     ]
    }
   ],
   "source": [
    "# specify parameters via map\n",
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'eta': 1,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 6,\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "# specify validations set to watch performance\n",
    "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "num_boost_round, early_stopping_rounds = 100, 10\n",
    "bst = xgb.train(param, dtrain, \n",
    "                num_boost_round=num_boost_round,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                evals=watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error=0.064663\n"
     ]
    }
   ],
   "source": [
    "# this is prediction\n",
    "preds = bst.predict(dtest)\n",
    "labels = dtest.get_label()\n",
    "print('error=%f' %\n",
    "      (sum(1 for i in range(len(preds)) if preds[i] != labels[i]) /\n",
    "       float(len(preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-mlogloss-mean</th>\n",
       "      <th>train-mlogloss-std</th>\n",
       "      <th>test-mlogloss-mean</th>\n",
       "      <th>test-mlogloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.426591</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.430054</td>\n",
       "      <td>0.004593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.275183</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.282285</td>\n",
       "      <td>0.007124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.208027</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.217758</td>\n",
       "      <td>0.005539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.177649</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.189222</td>\n",
       "      <td>0.004859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.159429</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.173555</td>\n",
       "      <td>0.004458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.147278</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.163131</td>\n",
       "      <td>0.003508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.138985</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.156991</td>\n",
       "      <td>0.004773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.133580</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.153485</td>\n",
       "      <td>0.004858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.129715</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.151260</td>\n",
       "      <td>0.005390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.126527</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.148825</td>\n",
       "      <td>0.004510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.123888</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.147654</td>\n",
       "      <td>0.004710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.121540</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.147534</td>\n",
       "      <td>0.005558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.119256</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.146672</td>\n",
       "      <td>0.004977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.117385</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.146516</td>\n",
       "      <td>0.005320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.115787</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.146430</td>\n",
       "      <td>0.005306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.114178</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.146516</td>\n",
       "      <td>0.005108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.112712</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.146304</td>\n",
       "      <td>0.005367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.111374</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.146292</td>\n",
       "      <td>0.005323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.110093</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.146336</td>\n",
       "      <td>0.005821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.108944</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.146310</td>\n",
       "      <td>0.006030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.107814</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.146180</td>\n",
       "      <td>0.006078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-mlogloss-mean  train-mlogloss-std  test-mlogloss-mean  \\\n",
       "0              0.426591            0.003592            0.430054   \n",
       "1              0.275183            0.000719            0.282285   \n",
       "2              0.208027            0.000892            0.217758   \n",
       "3              0.177649            0.001519            0.189222   \n",
       "4              0.159429            0.001682            0.173555   \n",
       "5              0.147278            0.002119            0.163131   \n",
       "6              0.138985            0.001722            0.156991   \n",
       "7              0.133580            0.001755            0.153485   \n",
       "8              0.129715            0.001684            0.151260   \n",
       "9              0.126527            0.002013            0.148825   \n",
       "10             0.123888            0.001866            0.147654   \n",
       "11             0.121540            0.001691            0.147534   \n",
       "12             0.119256            0.001332            0.146672   \n",
       "13             0.117385            0.001399            0.146516   \n",
       "14             0.115787            0.001446            0.146430   \n",
       "15             0.114178            0.001301            0.146516   \n",
       "16             0.112712            0.001287            0.146304   \n",
       "17             0.111374            0.001185            0.146292   \n",
       "18             0.110093            0.001119            0.146336   \n",
       "19             0.108944            0.001217            0.146310   \n",
       "20             0.107814            0.001232            0.146180   \n",
       "\n",
       "    test-mlogloss-std  \n",
       "0            0.004593  \n",
       "1            0.007124  \n",
       "2            0.005539  \n",
       "3            0.004859  \n",
       "4            0.004458  \n",
       "5            0.003508  \n",
       "6            0.004773  \n",
       "7            0.004858  \n",
       "8            0.005390  \n",
       "9            0.004510  \n",
       "10           0.004710  \n",
       "11           0.005558  \n",
       "12           0.004977  \n",
       "13           0.005320  \n",
       "14           0.005306  \n",
       "15           0.005108  \n",
       "16           0.005367  \n",
       "17           0.005323  \n",
       "18           0.005821  \n",
       "19           0.006030  \n",
       "20           0.006078  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify parameters via map\n",
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'eta': 1,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 6,\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "# specify validations set to watch performance\n",
    "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "num_boost_round, early_stopping_rounds = 200, 10\n",
    "cv_results = xgb.cv(param, dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    seed=42, nfold=5,\n",
    "                    metrics={'mlogloss'},\n",
    "                    early_stopping_rounds=10\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = 'merror'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=2, min_child_weight=4\n",
      "\tmerror 0.0573298 for 29 rounds\n",
      "CV with max_depth=2, min_child_weight=5\n",
      "\tmerror 0.057955999999999994 for 28 rounds\n",
      "CV with max_depth=2, min_child_weight=6\n",
      "\tmerror 0.0585258 for 21 rounds\n",
      "CV with max_depth=2, min_child_weight=7\n",
      "\tmerror 0.05732980000000001 for 34 rounds\n",
      "CV with max_depth=2, min_child_weight=8\n",
      "\tmerror 0.057842399999999995 for 33 rounds\n",
      "CV with max_depth=2, min_child_weight=9\n",
      "\tmerror 0.058525400000000005 for 17 rounds\n",
      "CV with max_depth=3, min_child_weight=4\n",
      "\tmerror 0.060973599999999996 for 12 rounds\n",
      "CV with max_depth=3, min_child_weight=5\n",
      "\tmerror 0.0610302 for 13 rounds\n",
      "CV with max_depth=3, min_child_weight=6\n",
      "\tmerror 0.05920879999999999 for 19 rounds\n",
      "CV with max_depth=3, min_child_weight=7\n",
      "\tmerror 0.0601764 for 24 rounds\n",
      "CV with max_depth=3, min_child_weight=8\n",
      "\tmerror 0.059778 for 21 rounds\n",
      "CV with max_depth=3, min_child_weight=9\n",
      "\tmerror 0.059892 for 13 rounds\n",
      "CV with max_depth=4, min_child_weight=4\n",
      "\tmerror 0.06256779999999999 for 11 rounds\n",
      "CV with max_depth=4, min_child_weight=5\n",
      "\tmerror 0.060745799999999996 for 11 rounds\n",
      "CV with max_depth=4, min_child_weight=6\n",
      "\tmerror 0.061144199999999996 for 7 rounds\n",
      "CV with max_depth=4, min_child_weight=7\n",
      "\tmerror 0.0606888 for 13 rounds\n",
      "CV with max_depth=4, min_child_weight=8\n",
      "\tmerror 0.059436199999999995 for 16 rounds\n",
      "CV with max_depth=4, min_child_weight=9\n",
      "\tmerror 0.060802800000000004 for 5 rounds\n",
      "CV with max_depth=5, min_child_weight=4\n",
      "\tmerror 0.061941399999999994 for 7 rounds\n",
      "CV with max_depth=5, min_child_weight=5\n",
      "\tmerror 0.061429199999999996 for 7 rounds\n",
      "CV with max_depth=5, min_child_weight=6\n",
      "\tmerror 0.061486000000000006 for 9 rounds\n",
      "CV with max_depth=5, min_child_weight=7\n",
      "\tmerror 0.061314999999999995 for 7 rounds\n",
      "CV with max_depth=5, min_child_weight=8\n",
      "\tmerror 0.0618274 for 6 rounds\n",
      "CV with max_depth=5, min_child_weight=9\n",
      "\tmerror 0.061599799999999996 for 5 rounds\n",
      "CV with max_depth=6, min_child_weight=4\n",
      "\tmerror 0.061543 for 9 rounds\n",
      "CV with max_depth=6, min_child_weight=5\n",
      "\tmerror 0.06245379999999999 for 5 rounds\n",
      "CV with max_depth=6, min_child_weight=6\n",
      "\tmerror 0.06216900000000001 for 3 rounds\n",
      "CV with max_depth=6, min_child_weight=7\n",
      "\tmerror 0.0627954 for 5 rounds\n",
      "CV with max_depth=6, min_child_weight=8\n",
      "\tmerror 0.062168999999999995 for 4 rounds\n",
      "CV with max_depth=6, min_child_weight=9\n",
      "\tmerror 0.062168999999999995 for 10 rounds\n",
      "CV with max_depth=7, min_child_weight=4\n",
      "\tmerror 0.063137 for 5 rounds\n",
      "CV with max_depth=7, min_child_weight=5\n",
      "\tmerror 0.0635352 for 4 rounds\n",
      "CV with max_depth=7, min_child_weight=6\n",
      "\tmerror 0.0618276 for 3 rounds\n",
      "CV with max_depth=7, min_child_weight=7\n",
      "\tmerror 0.062283 for 5 rounds\n",
      "CV with max_depth=7, min_child_weight=8\n",
      "\tmerror 0.0618278 for 3 rounds\n",
      "CV with max_depth=7, min_child_weight=9\n",
      "\tmerror 0.062283 for 4 rounds\n",
      "Best params: 2, 4, merror: 0.0573298\n"
     ]
    }
   ],
   "source": [
    "# You can try wider intervals with a larger step between\n",
    "# each value and then narrow it down. Here after several\n",
    "# iteration I found that the optimal value was in the\n",
    "# following ranges.\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight) for max_depth in range(2, 8) for min_child_weight in range(4, 10)\n",
    "]\n",
    "param = {\n",
    "    'eta': 1,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 6,\n",
    "    'eval_metric': loss_function\n",
    "}\n",
    "\n",
    "\n",
    "# Define initial best params and MAE\n",
    "min_loss = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(f\"CV with max_depth={max_depth}, min_child_weight={min_child_weight}\")\n",
    "    \n",
    "    # Update our parameters\n",
    "    param['max_depth'] = max_depth\n",
    "    param['min_child_weight'] = min_child_weight\n",
    "    \n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(param,dtrain,\n",
    "                        num_boost_round=num_boost_round,\n",
    "                        seed=42, nfold=5,\n",
    "                        metrics={loss_function},\n",
    "                        early_stopping_rounds=10)\n",
    "\n",
    "    # Update best loss\n",
    "    mean_loss = cv_results[f'test-{loss_function}-mean'].min()\n",
    "    boost_rounds = cv_results[f'test-{loss_function}-mean'].argmin()\n",
    "    print(f\"\\t{loss_function} {mean_loss} for {boost_rounds} rounds\")\n",
    "    if mean_loss < min_loss:\n",
    "        min_loss = mean_loss\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "print(f\"Best params: {best_params[0]}, {best_params[1]}, {loss_function}: {min_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best params: max_depth: 2, min_child_weight: 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample=1.0\n",
      "\tmerror 0.05732980000000001 for 34 rounds\n",
      "CV with subsample=1.0, colsample=0.9\n",
      "\tmerror 0.0576716 for 22 rounds\n",
      "CV with subsample=1.0, colsample=0.8\n",
      "\tmerror 0.057216 for 41 rounds\n",
      "CV with subsample=1.0, colsample=0.7\n",
      "\tmerror 0.06023339999999999 for 13 rounds\n",
      "CV with subsample=0.9, colsample=1.0\n",
      "\tmerror 0.0574436 for 43 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n",
      "\tmerror 0.05886739999999999 for 29 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "\tmerror 0.0571592 for 18 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n",
      "\tmerror 0.058013 for 30 rounds\n",
      "CV with subsample=0.8, colsample=1.0\n",
      "\tmerror 0.058297600000000005 for 29 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "\tmerror 0.06046119999999999 for 16 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n",
      "\tmerror 0.0589812 for 22 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "\tmerror 0.05818399999999999 for 16 rounds\n",
      "CV with subsample=0.7, colsample=1.0\n",
      "\tmerror 0.058753 for 19 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "\tmerror 0.058753 for 27 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n",
      "\tmerror 0.0578424 for 17 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "\tmerror 0.05960719999999999 for 19 rounds\n",
      "Best params: 0.9, 0.8, merror: 0.0571592\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]\n",
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'min_child_weight': 7,\n",
    "    'eta': 1,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 6,\n",
    "    'eval_metric': loss_function\n",
    "}\n",
    "\n",
    "min_loss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(f\"CV with subsample={subsample}, colsample={colsample}\")\n",
    "\n",
    "    # We update our parameters\n",
    "    param['subsample'] = subsample\n",
    "    param['colsample_bytree'] = colsample\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(param, dtrain,\n",
    "                        num_boost_round=num_boost_round,\n",
    "                        seed=42, nfold=5,\n",
    "                        metrics={loss_function},\n",
    "                        early_stopping_rounds=10)\n",
    "\n",
    "    # Update best score\n",
    "    mean_loss = cv_results[f'test-{loss_function}-mean'].min()\n",
    "    boost_rounds = cv_results[f'test-{loss_function}-mean'].argmin()\n",
    "    print(f\"\\t{loss_function} {mean_loss} for {boost_rounds} rounds\")\n",
    "    if mean_loss < min_loss:\n",
    "        min_loss = mean_loss\n",
    "        best_params = (subsample, colsample)\n",
    "print(f\"Best params: {best_params[0]}, {best_params[1]}, {loss_function}: {min_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best params: subsample: 0.9, colsample: 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n",
      "CV with eta=1\n",
      "CPU times: user 15.5 s, sys: 171 ms, total: 15.7 s\n",
      "Wall time: 4.28 s\n",
      "\tmerror 0.059778 for 25 rounds\n",
      "\n",
      "CV with eta=0.5\n",
      "CPU times: user 17.2 s, sys: 123 ms, total: 17.3 s\n",
      "Wall time: 4.48 s\n",
      "\tmerror 0.0575576 for 26 rounds\n",
      "\n",
      "CV with eta=0.3\n",
      "CPU times: user 32.3 s, sys: 252 ms, total: 32.5 s\n",
      "Wall time: 8.58 s\n",
      "\tmerror 0.0577284 for 57 rounds\n",
      "\n",
      "CV with eta=0.2\n",
      "CPU times: user 46.9 s, sys: 309 ms, total: 47.2 s\n",
      "Wall time: 12.3 s\n",
      "\tmerror 0.056874400000000006 for 96 rounds\n",
      "\n",
      "CV with eta=0.1\n",
      "CPU times: user 48.4 s, sys: 506 ms, total: 48.9 s\n",
      "Wall time: 13.7 s\n",
      "\tmerror 0.05915159999999999 for 95 rounds\n",
      "\n",
      "CV with eta=0.05\n",
      "CPU times: user 48.2 s, sys: 570 ms, total: 48.8 s\n",
      "Wall time: 13.5 s\n",
      "\tmerror 0.063194 for 99 rounds\n",
      "\n",
      "CV with eta=0.01\n",
      "CPU times: user 8.02 s, sys: 94.4 ms, total: 8.11 s\n",
      "Wall time: 2.19 s\n",
      "\tmerror 0.100427 for 5 rounds\n",
      "\n",
      "CV with eta=0.005\n",
      "CPU times: user 8.3 s, sys: 88.5 ms, total: 8.39 s\n",
      "Wall time: 2.22 s\n",
      "\tmerror 0.09957300000000001 for 5 rounds\n",
      "\n",
      "Best params: 0.2, MLOGLOSS: 0.056874400000000006\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'min_child_weight': 7,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 6,\n",
    "    'eval_metric': loss_function\n",
    "}\n",
    "\n",
    "min_loss = float(\"Inf\")\n",
    "best_params = None\n",
    "for eta in [1, .5, .3, .2, .1, .05, .01, .005]:\n",
    "    print(f\"CV with eta={eta}\")\n",
    "    # We update our parameters\n",
    "    param['eta'] = eta\n",
    "    \n",
    "    # Run and time CV\n",
    "    %time cv_results = xgb.cv(param, dtrain, \\\n",
    "                              num_boost_round=num_boost_round, \\\n",
    "                              seed=42, nfold=5, \\\n",
    "                              metrics=[loss_function], \\\n",
    "                              early_stopping_rounds=10)\n",
    "\n",
    "    # Update best score\n",
    "    mean_loss = cv_results[f'test-{loss_function}-mean'].min()\n",
    "    boost_rounds = cv_results[f'test-{loss_function}-mean'].argmin()\n",
    "    print(f\"\\t{loss_function} {mean_loss} for {boost_rounds} rounds\\n\")\n",
    "    if mean_loss < min_loss:\n",
    "        min_loss = mean_loss\n",
    "        best_params = eta\n",
    "print(f\"Best params: {best_params}, MLOGLOSS: {min_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best params: eta: 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.09177\ttest-merror:0.09335\n",
      "Multiple eval metrics have been passed: 'test-merror' will be used for early stopping.\n",
      "\n",
      "Will train until test-merror hasn't improved in 10 rounds.\n",
      "[1]\ttrain-merror:0.10760\ttest-merror:0.10974\n",
      "[2]\ttrain-merror:0.08876\ttest-merror:0.09039\n",
      "[3]\ttrain-merror:0.08244\ttest-merror:0.08515\n",
      "[4]\ttrain-merror:0.08249\ttest-merror:0.08333\n",
      "[5]\ttrain-merror:0.08244\ttest-merror:0.08288\n",
      "[6]\ttrain-merror:0.07896\ttest-merror:0.07832\n",
      "[7]\ttrain-merror:0.07526\ttest-merror:0.07650\n",
      "[8]\ttrain-merror:0.07532\ttest-merror:0.07605\n",
      "[9]\ttrain-merror:0.07339\ttest-merror:0.07514\n",
      "[10]\ttrain-merror:0.07355\ttest-merror:0.07514\n",
      "[11]\ttrain-merror:0.07185\ttest-merror:0.07468\n",
      "[12]\ttrain-merror:0.06946\ttest-merror:0.07240\n",
      "[13]\ttrain-merror:0.06877\ttest-merror:0.07218\n",
      "[14]\ttrain-merror:0.06764\ttest-merror:0.07195\n",
      "[15]\ttrain-merror:0.06644\ttest-merror:0.07081\n",
      "[16]\ttrain-merror:0.06667\ttest-merror:0.07104\n",
      "[17]\ttrain-merror:0.06598\ttest-merror:0.06967\n",
      "[18]\ttrain-merror:0.06490\ttest-merror:0.06899\n",
      "[19]\ttrain-merror:0.06376\ttest-merror:0.06717\n",
      "[20]\ttrain-merror:0.06302\ttest-merror:0.06535\n",
      "[21]\ttrain-merror:0.06046\ttest-merror:0.06284\n",
      "[22]\ttrain-merror:0.05932\ttest-merror:0.06079\n",
      "[23]\ttrain-merror:0.05892\ttest-merror:0.06170\n",
      "[24]\ttrain-merror:0.05847\ttest-merror:0.06079\n",
      "[25]\ttrain-merror:0.05767\ttest-merror:0.06011\n",
      "[26]\ttrain-merror:0.05699\ttest-merror:0.06011\n",
      "[27]\ttrain-merror:0.05716\ttest-merror:0.06011\n",
      "[28]\ttrain-merror:0.05733\ttest-merror:0.06057\n",
      "[29]\ttrain-merror:0.05676\ttest-merror:0.05988\n",
      "[30]\ttrain-merror:0.05625\ttest-merror:0.05943\n",
      "[31]\ttrain-merror:0.05619\ttest-merror:0.05988\n",
      "[32]\ttrain-merror:0.05579\ttest-merror:0.05943\n",
      "[33]\ttrain-merror:0.05574\ttest-merror:0.05920\n",
      "[34]\ttrain-merror:0.05511\ttest-merror:0.05920\n",
      "[35]\ttrain-merror:0.05528\ttest-merror:0.05851\n",
      "[36]\ttrain-merror:0.05477\ttest-merror:0.05920\n",
      "[37]\ttrain-merror:0.05414\ttest-merror:0.05851\n",
      "[38]\ttrain-merror:0.05437\ttest-merror:0.05874\n",
      "[39]\ttrain-merror:0.05420\ttest-merror:0.05988\n",
      "[40]\ttrain-merror:0.05391\ttest-merror:0.05965\n",
      "[41]\ttrain-merror:0.05369\ttest-merror:0.06057\n",
      "[42]\ttrain-merror:0.05323\ttest-merror:0.06011\n",
      "[43]\ttrain-merror:0.05346\ttest-merror:0.05920\n",
      "[44]\ttrain-merror:0.05300\ttest-merror:0.05897\n",
      "[45]\ttrain-merror:0.05266\ttest-merror:0.05897\n",
      "Stopping. Best iteration:\n",
      "[35]\ttrain-merror:0.05528\ttest-merror:0.05851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'min_child_weight': 7,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 6,\n",
    "    'eval_metric': loss_function,\n",
    "    'eta': 0.2\n",
    "}\n",
    "\n",
    "num_boost_round, early_stopping_rounds = 200, 10\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "model = xgb.train(param, dtrain,\n",
    "                  num_boost_round=num_boost_round,\n",
    "                  early_stopping_rounds=10,\n",
    "                  evals=watchlist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.10548\n",
      "Will train until train-merror hasn't improved in 10 rounds.\n",
      "[1]\ttrain-merror:0.10361\n",
      "[2]\ttrain-merror:0.10193\n",
      "[3]\ttrain-merror:0.08754\n",
      "[4]\ttrain-merror:0.08467\n",
      "[5]\ttrain-merror:0.08079\n",
      "[6]\ttrain-merror:0.08184\n",
      "[7]\ttrain-merror:0.07911\n",
      "[8]\ttrain-merror:0.07815\n",
      "[9]\ttrain-merror:0.07578\n",
      "[10]\ttrain-merror:0.07478\n",
      "[11]\ttrain-merror:0.07178\n",
      "[12]\ttrain-merror:0.06950\n",
      "[13]\ttrain-merror:0.06895\n",
      "[14]\ttrain-merror:0.06886\n",
      "[15]\ttrain-merror:0.06827\n",
      "[16]\ttrain-merror:0.06754\n",
      "[17]\ttrain-merror:0.06608\n",
      "[18]\ttrain-merror:0.06613\n",
      "[19]\ttrain-merror:0.06458\n",
      "[20]\ttrain-merror:0.06331\n",
      "[21]\ttrain-merror:0.06153\n",
      "[22]\ttrain-merror:0.05984\n",
      "[23]\ttrain-merror:0.05953\n",
      "[24]\ttrain-merror:0.05925\n",
      "[25]\ttrain-merror:0.05871\n",
      "[26]\ttrain-merror:0.05839\n",
      "[27]\ttrain-merror:0.05811\n",
      "[28]\ttrain-merror:0.05770\n",
      "[29]\ttrain-merror:0.05779\n",
      "[30]\ttrain-merror:0.05775\n",
      "[31]\ttrain-merror:0.05679\n",
      "[32]\ttrain-merror:0.05707\n",
      "[33]\ttrain-merror:0.05625\n",
      "[34]\ttrain-merror:0.05588\n",
      "[35]\ttrain-merror:0.05588\n",
      "[36]\ttrain-merror:0.05547\n",
      "[37]\ttrain-merror:0.05534\n",
      "[38]\ttrain-merror:0.05515\n",
      "[39]\ttrain-merror:0.05456\n",
      "[40]\ttrain-merror:0.05424\n",
      "[41]\ttrain-merror:0.05415\n",
      "[42]\ttrain-merror:0.05411\n",
      "[43]\ttrain-merror:0.05388\n",
      "[44]\ttrain-merror:0.05383\n",
      "[45]\ttrain-merror:0.05365\n",
      "[46]\ttrain-merror:0.05319\n",
      "[47]\ttrain-merror:0.05319\n",
      "[48]\ttrain-merror:0.05269\n",
      "[49]\ttrain-merror:0.05288\n",
      "[50]\ttrain-merror:0.05242\n",
      "[51]\ttrain-merror:0.05219\n",
      "[52]\ttrain-merror:0.05196\n",
      "[53]\ttrain-merror:0.05165\n",
      "[54]\ttrain-merror:0.05183\n",
      "[55]\ttrain-merror:0.05142\n",
      "[56]\ttrain-merror:0.05124\n",
      "[57]\ttrain-merror:0.05110\n",
      "[58]\ttrain-merror:0.05101\n",
      "[59]\ttrain-merror:0.05074\n",
      "[60]\ttrain-merror:0.05074\n",
      "[61]\ttrain-merror:0.05046\n",
      "[62]\ttrain-merror:0.05042\n",
      "[63]\ttrain-merror:0.05014\n",
      "[64]\ttrain-merror:0.05014\n",
      "[65]\ttrain-merror:0.05005\n",
      "[66]\ttrain-merror:0.05001\n",
      "[67]\ttrain-merror:0.04978\n",
      "[68]\ttrain-merror:0.04960\n",
      "[69]\ttrain-merror:0.04932\n",
      "[70]\ttrain-merror:0.04928\n",
      "[71]\ttrain-merror:0.04900\n",
      "[72]\ttrain-merror:0.04928\n",
      "[73]\ttrain-merror:0.04928\n",
      "[74]\ttrain-merror:0.04932\n",
      "[75]\ttrain-merror:0.04932\n",
      "[76]\ttrain-merror:0.04928\n",
      "[77]\ttrain-merror:0.04891\n",
      "[78]\ttrain-merror:0.04905\n",
      "[79]\ttrain-merror:0.04896\n",
      "[80]\ttrain-merror:0.04859\n",
      "[81]\ttrain-merror:0.04878\n",
      "[82]\ttrain-merror:0.04859\n",
      "[83]\ttrain-merror:0.04837\n",
      "[84]\ttrain-merror:0.04837\n",
      "[85]\ttrain-merror:0.04818\n",
      "[86]\ttrain-merror:0.04796\n",
      "[87]\ttrain-merror:0.04805\n",
      "[88]\ttrain-merror:0.04800\n",
      "[89]\ttrain-merror:0.04791\n",
      "[90]\ttrain-merror:0.04773\n",
      "[91]\ttrain-merror:0.04782\n",
      "[92]\ttrain-merror:0.04777\n",
      "[93]\ttrain-merror:0.04777\n",
      "[94]\ttrain-merror:0.04759\n",
      "[95]\ttrain-merror:0.04732\n",
      "[96]\ttrain-merror:0.04741\n",
      "[97]\ttrain-merror:0.04723\n",
      "[98]\ttrain-merror:0.04718\n",
      "[99]\ttrain-merror:0.04709\n",
      "[100]\ttrain-merror:0.04705\n",
      "[101]\ttrain-merror:0.04695\n",
      "[102]\ttrain-merror:0.04686\n",
      "[103]\ttrain-merror:0.04673\n",
      "[104]\ttrain-merror:0.04686\n",
      "[105]\ttrain-merror:0.04673\n",
      "[106]\ttrain-merror:0.04668\n",
      "[107]\ttrain-merror:0.04677\n",
      "[108]\ttrain-merror:0.04664\n",
      "[109]\ttrain-merror:0.04668\n",
      "[110]\ttrain-merror:0.04659\n",
      "[111]\ttrain-merror:0.04650\n",
      "[112]\ttrain-merror:0.04614\n",
      "[113]\ttrain-merror:0.04614\n",
      "[114]\ttrain-merror:0.04600\n",
      "[115]\ttrain-merror:0.04600\n",
      "[116]\ttrain-merror:0.04582\n",
      "[117]\ttrain-merror:0.04586\n",
      "[118]\ttrain-merror:0.04573\n",
      "[119]\ttrain-merror:0.04582\n",
      "[120]\ttrain-merror:0.04573\n",
      "[121]\ttrain-merror:0.04577\n",
      "[122]\ttrain-merror:0.04568\n",
      "[123]\ttrain-merror:0.04573\n",
      "[124]\ttrain-merror:0.04564\n",
      "[125]\ttrain-merror:0.04545\n",
      "[126]\ttrain-merror:0.04536\n",
      "[127]\ttrain-merror:0.04527\n",
      "[128]\ttrain-merror:0.04527\n",
      "[129]\ttrain-merror:0.04545\n",
      "[130]\ttrain-merror:0.04536\n",
      "[131]\ttrain-merror:0.04504\n",
      "[132]\ttrain-merror:0.04500\n",
      "[133]\ttrain-merror:0.04513\n",
      "[134]\ttrain-merror:0.04509\n",
      "[135]\ttrain-merror:0.04495\n",
      "[136]\ttrain-merror:0.04495\n",
      "[137]\ttrain-merror:0.04468\n",
      "[138]\ttrain-merror:0.04468\n",
      "[139]\ttrain-merror:0.04463\n",
      "[140]\ttrain-merror:0.04472\n",
      "[141]\ttrain-merror:0.04468\n",
      "[142]\ttrain-merror:0.04463\n",
      "[143]\ttrain-merror:0.04459\n",
      "[144]\ttrain-merror:0.04463\n",
      "[145]\ttrain-merror:0.04468\n",
      "[146]\ttrain-merror:0.04468\n",
      "[147]\ttrain-merror:0.04459\n",
      "[148]\ttrain-merror:0.04454\n",
      "[149]\ttrain-merror:0.04450\n",
      "[150]\ttrain-merror:0.04468\n",
      "[151]\ttrain-merror:0.04445\n",
      "[152]\ttrain-merror:0.04450\n",
      "[153]\ttrain-merror:0.04454\n",
      "[154]\ttrain-merror:0.04454\n",
      "[155]\ttrain-merror:0.04427\n",
      "[156]\ttrain-merror:0.04441\n",
      "[157]\ttrain-merror:0.04436\n",
      "[158]\ttrain-merror:0.04418\n",
      "[159]\ttrain-merror:0.04404\n",
      "[160]\ttrain-merror:0.04381\n",
      "[161]\ttrain-merror:0.04377\n",
      "[162]\ttrain-merror:0.04363\n",
      "[163]\ttrain-merror:0.04363\n",
      "[164]\ttrain-merror:0.04349\n",
      "[165]\ttrain-merror:0.04349\n",
      "[166]\ttrain-merror:0.04354\n",
      "[167]\ttrain-merror:0.04336\n",
      "[168]\ttrain-merror:0.04331\n",
      "[169]\ttrain-merror:0.04340\n",
      "[170]\ttrain-merror:0.04317\n",
      "[171]\ttrain-merror:0.04304\n",
      "[172]\ttrain-merror:0.04304\n",
      "[173]\ttrain-merror:0.04295\n",
      "[174]\ttrain-merror:0.04308\n",
      "[175]\ttrain-merror:0.04258\n",
      "[176]\ttrain-merror:0.04254\n",
      "[177]\ttrain-merror:0.04226\n",
      "[178]\ttrain-merror:0.04249\n",
      "[179]\ttrain-merror:0.04245\n",
      "[180]\ttrain-merror:0.04263\n",
      "[181]\ttrain-merror:0.04245\n",
      "[182]\ttrain-merror:0.04254\n",
      "[183]\ttrain-merror:0.04226\n",
      "[184]\ttrain-merror:0.04231\n",
      "[185]\ttrain-merror:0.04231\n",
      "[186]\ttrain-merror:0.04254\n",
      "[187]\ttrain-merror:0.04258\n",
      "Stopping. Best iteration:\n",
      "[177]\ttrain-merror:0.04226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'min_child_weight': 7,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 6,\n",
    "    'eval_metric': loss_function,\n",
    "    'eta': 0.2\n",
    "}\n",
    "\n",
    "num_boost_round, early_stopping_rounds = 200, 10\n",
    "watchlist = [(full_dtrain, 'train')]\n",
    "model = xgb.train(param, full_dtrain,\n",
    "                  num_boost_round=num_boost_round,\n",
    "                  early_stopping_rounds=10,\n",
    "                  evals=watchlist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error=0.044854\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(dtest)\n",
    "labels = dtest.get_label()\n",
    "print('error=%f' %\n",
    "      (sum(1 for i in range(len(preds)) if preds[i] != labels[i]) /\n",
    "       float(len(preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(full_dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'id': test_data.id, 'label': preds_test}).round().astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('submission_19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.000e-05 1.548e-02]\n",
      "[0.0157 0.0179]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[0.91, 0.66], [0.46, 0.33]])\n",
    "b = np.array([0.25, 0.14])\n",
    "\n",
    "x_1 = np.array([-0.088, 0.5])\n",
    "x_2 = np.array([0.99, -1.01])\n",
    "print(b-A@x_1)\n",
    "print(b-A@x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
